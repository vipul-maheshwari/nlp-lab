{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmJ9N1blcHXP"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import requests\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from nltk.corpus import treebank, brown, conll2000\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVbpx-8_tSrD"
      },
      "source": [
        "# Part-of-Speech Tagging with a Bidirectional LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LVEGU9cta45"
      },
      "source": [
        "It's difficult to find free sequence labelling datasets because they're so labour-intensive to create.\n",
        "<br><br>\n",
        "Fortunately, **Natural Language Toolkit (NLTK)** includes enough free sets of labelled corpora for our purposes. NLTK also provides them in a convenient uniform format.<br>\n",
        "https://www.nltk.org/index.html<br>\n",
        "https://www.nltk.org/nltk_data/<br>\n",
        "<br>\n",
        "We'll use the Treebank, Brown, and CONLL-2000 datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TawHhQnsENGX"
      },
      "outputs": [],
      "source": [
        "nltk.download('treebank')\n",
        "nltk.download('brown')\n",
        "nltk.download('conll2000')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1AMD2ojvtSU"
      },
      "source": [
        "In their original form, the datasets use different part-of-speech (PoS) tag sets. We need to ensure they all use the same tagset, so we'll download a simplified set called the *universal_tagset* from NLTK.<br>\n",
        "\n",
        "See Section 2.3 here for a list of tags: https://www.nltk.org/book/ch05.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ska6B_5vs0s"
      },
      "outputs": [],
      "source": [
        "nltk.download('universal_tagset')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzvREp9EwNld"
      },
      "source": [
        "We'll then retrieve the tagged sentences from each dataset, taking care to specify they should use the *universal tagset* we just downloaded. We'll then combine them into one collection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srteMIhDEbRS"
      },
      "outputs": [],
      "source": [
        "# Download all PoS-tagged sentences and place them in one list.\n",
        "tagged_sentences = treebank.tagged_sents(tagset='universal') +\\\n",
        "                   brown.tagged_sents(tagset='universal') +\\\n",
        "                   conll2000.tagged_sents(tagset='universal')\n",
        "\n",
        "print(tagged_sentences[0])\n",
        "print(f\"Dataset size: {len(tagged_sentences)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY9upptpw6fi"
      },
      "source": [
        "Each tagged sentence is actually a list of word-tag tuples (bear in mind that NLTK's universal tagset is a reduced tagset so items such as *proper nouns* are simply tagged as *nouns*).<br>\n",
        "\n",
        "Our model is going to take in a sequence of words, and output a sequence of PoS tags, so we need to separate the words from the tags in our dataset. The tag sequences will serve as our training labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7unpKPbNOUE"
      },
      "outputs": [],
      "source": [
        "sentences, sentence_tags = [], []\n",
        "\n",
        "for s in tagged_sentences:\n",
        "  sentence, tags = zip(*s)\n",
        "  sentences.append(list(sentence))\n",
        "  sentence_tags.append(list(tags))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzK70wbrH71p"
      },
      "source": [
        "The sentences and their respective tags are now in separate lists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Im5-zeiE1jk"
      },
      "outputs": [],
      "source": [
        "print(sentences[0])\n",
        "print(sentence_tags[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyYJQufRi_4l"
      },
      "outputs": [],
      "source": [
        "print(len(sentences), len(sentence_tags))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sL7Voept0lEi"
      },
      "source": [
        "Create train/validation/test splits. This time, we don't have a separate test set so we'll call *train_test_split* twice.<br>\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dqmencun067o"
      },
      "outputs": [],
      "source": [
        "train_ratio = 0.75\n",
        "validation_ratio = 0.15\n",
        "test_ratio = 0.10\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(sentences, sentence_tags,\n",
        "                                                    test_size=1 - train_ratio,\n",
        "                                                    random_state=1)\n",
        "\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_test, y_test,\n",
        "                                                test_size=test_ratio/(test_ratio + validation_ratio),\n",
        "                                                random_state=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRPQyAgfR5sm"
      },
      "outputs": [],
      "source": [
        "print(len(x_train), len(y_train))\n",
        "print(len(x_val), len(y_val))\n",
        "print(len(x_test), len(y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joyhlIqixpCY"
      },
      "source": [
        "Now that we have our datasets preprocessed, the next step is to vectorize. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kT5VFgpRxlzO"
      },
      "source": [
        "First, we need to create a tokenizer for the sentences and *fit* it to the training dataset to create a vocabulary. We'll just use the default tokenizer settings which applies some light filtering, lowers the case, and separates on spaces. We'll also supply an out-of-vocabulary token (\\<OOV\\>) in case the tokenizer encounters words during testing/inference which it doesn't during training.<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CV1gT-GmOOI9"
      },
      "outputs": [],
      "source": [
        "sentence_tokenizer = keras.preprocessing.text.Tokenizer(oov_token='<OOV>')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMmMUDogOPoe"
      },
      "outputs": [],
      "source": [
        "sentence_tokenizer.fit_on_texts(x_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziy90iSwORAX"
      },
      "outputs": [],
      "source": [
        "print(f\"Vocabulary size: {len(sentence_tokenizer.word_index)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5icc4yz_yWRD"
      },
      "source": [
        "We also need to create *another* tokenizer for the tags since our labels are also sequences. This time, we won't need an OOV token because there are only a handful of tags and, in this case, they'll all be encountered during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3H571o7bOSQ2"
      },
      "outputs": [],
      "source": [
        "tag_tokenizer = keras.preprocessing.text.Tokenizer()\n",
        "tag_tokenizer.fit_on_texts(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fDtT7_YOhoA"
      },
      "outputs": [],
      "source": [
        "print(f\"Number of PoS tags: {len(tag_tokenizer.word_index)}\\n\")\n",
        "tag_tokenizer.get_config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJrIrOwzyddJ"
      },
      "outputs": [],
      "source": [
        "# The set of universal PoS tags.\n",
        "tag_tokenizer.word_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUE5rf_myuGk"
      },
      "source": [
        "Next, we need to vectorize our sentences and corresponding tags, we'll use the tokenizer's *texts_to_sequences* method to convert each sentence to a sequence of integers where each integer maps to a particular token.<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#texts_to_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Khb0e73Uy9F2"
      },
      "outputs": [],
      "source": [
        "x_train_seqs = sentence_tokenizer.texts_to_sequences(x_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3A5T7HDlPG04"
      },
      "outputs": [],
      "source": [
        "print(x_train_seqs[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77U2nCHKLC4z"
      },
      "source": [
        "We can use the *sequences_to_texts* method to convert a vectorized sentence back to its preprocessed form.<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#sequences_to_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Io3WhTXPPcBY"
      },
      "outputs": [],
      "source": [
        "print(f\"Original: {x_train[0]}\")\n",
        "print(f\"Reconstructed: {sentence_tokenizer.sequences_to_texts([x_train_seqs[0]])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrTae0CTLbzs"
      },
      "source": [
        "Next, we'll vectorize the labels (i.e. sequences of PoS tags) using its respective tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "px0r8Z8cPhqS"
      },
      "outputs": [],
      "source": [
        "y_train_seqs = tag_tokenizer.texts_to_sequences(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qzz8TCP6y6LM"
      },
      "outputs": [],
      "source": [
        "tag_tokenizer.sequences_to_texts([y_train_seqs[0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzMIDKL2Lov7"
      },
      "source": [
        "Finally, we'll do the same with the validation inputs and labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZKg10BX5O-N"
      },
      "outputs": [],
      "source": [
        "x_val_seqs = sentence_tokenizer.texts_to_sequences(x_val)\n",
        "y_val_seqs = tag_tokenizer.texts_to_sequences(y_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfT-5dmCy_uJ"
      },
      "source": [
        "As we covered in the slides, **Recurrent Neural Networks** are capable of handling variable length sequences.<br><br>\n",
        "Despite that, it's still best to pad or truncate sequences to a uniform length for one or both of these reasons:<br>\n",
        "1. Performance. The longer a sequence, the higher the computation cost. One may want to truncate long sequences to a shorter length if that's feasible and doesn't result in too much performance loss.\n",
        "\n",
        "2. When processing datasets in batches, each sequence *in a batch* usually has to be of uniform length.<br>\n",
        "\n",
        "For simplicity, here, we'll make *every* sequence be as long as the longest sequence. In other words, we'll determine how long the longest sequence is, then pad out the rest of the sequences to be the same length.<br>\n",
        "\n",
        "A more optimized solution would be to make each sequence as long as the longest sequence in each *batch* to avoid unnecessary processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYLLph5iPng5"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = len(max(x_train_seqs, key=len))\n",
        "print(f\"Length of longest input sequence: {MAX_LENGTH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khQdjjfOQ5wc"
      },
      "source": [
        "We can pad the sequences with the *pad_sequences* method:<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2F9536qPpbI"
      },
      "outputs": [],
      "source": [
        "x_train_padded = keras.preprocessing.sequence.pad_sequences(x_train_seqs, padding='post',\n",
        "                                                            maxlen=MAX_LENGTH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BKa6RNLPy1J"
      },
      "outputs": [],
      "source": [
        "print(x_train_padded[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpbsTGrgRGNR"
      },
      "source": [
        "We'll do the same with the training label (PoS sequences)..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDTo19V_P0CB"
      },
      "outputs": [],
      "source": [
        "y_train_padded = keras.preprocessing.sequence.pad_sequences(y_train_seqs, padding='post',\n",
        "                                                            maxlen=MAX_LENGTH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4bOcVgLRK4b"
      },
      "source": [
        "...and the validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCtKDmLK5uq4"
      },
      "outputs": [],
      "source": [
        "x_val_padded = keras.preprocessing.sequence.pad_sequences(x_val_seqs, padding='post', maxlen=MAX_LENGTH)\n",
        "y_val_padded = keras.preprocessing.sequence.pad_sequences(y_val_seqs, padding='post', maxlen=MAX_LENGTH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFQqWmck25sA"
      },
      "source": [
        "PoS tagging is a multiclass classification task done at each timestep, so we need to convert every tag for every sentence into a one-hot encoding<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZNu8P_nSv3u"
      },
      "outputs": [],
      "source": [
        "y_train_categoricals = keras.utils.to_categorical(y_train_padded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_Reegcq3iiJ"
      },
      "source": [
        "The label (PoS tag sequence) for a single sentence is now a **sequence of one-hot encodings**. These will serve as our training targets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPJlV1P_UStC"
      },
      "outputs": [],
      "source": [
        "# The one hot encodings for the first label.\n",
        "print(y_train_categoricals[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wzIkD9L3pz6"
      },
      "outputs": [],
      "source": [
        "# One-hot encoding for a single tag.\n",
        "print(y_train_categoricals[0][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIxLTetGStWg"
      },
      "source": [
        "We can determine the PoS tag from a one-hot encoding by seeing which index is set to 1, then using that to query the tag tokenizer's *index_word* dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAqLUMuw319d"
      },
      "outputs": [],
      "source": [
        "idx = np.argmax(y_train_categoricals[0][0])\n",
        "print(f\"Index: {idx}\")\n",
        "\n",
        "print(f\"Tag: {tag_tokenizer.index_word[idx]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiZFnLm5TWj3"
      },
      "source": [
        "We'll one-hot encode the validation labels as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZMDn8E2357_"
      },
      "outputs": [],
      "source": [
        "y_val_categoricals = keras.utils.to_categorical(y_val_padded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdzikQR26NS2"
      },
      "source": [
        "At this point, we're ready to build our model. We'll train word embeddings concurrently with our model \n",
        "There are several new things here:<br>\n",
        "1. The embedding layer has a *mask_zero* parameter. We added padding in order to make our batches the same size, but we don't want the model to make PoS predictions on padding. Setting *mask_zero* to *True* makes the layers following the embedding layer ignore padding values.<br>\n",
        "https://www.tensorflow.org/guide/keras/masking_and_padding<br>\n",
        "https://stackoverflow.com/questions/47485216/how-does-mask-zero-in-keras-embedding-layer-work<br><br>\n",
        "2. We're using a **bidirectional LSTM**. The *Bidirectional* layer is a wrapper to which we pass an *LSTM* layer. The first parameter to the *LSTM* layer is the number of units in the cell. The second parameter, *return_sequences*, controls whether the RNN returns an output for each timestep or only the last output. Since we're doing PoS-tagging, we want an output for each timestep and so *return_sequences* is set to *True*.<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96PD9ncGQqO7"
      },
      "outputs": [],
      "source": [
        "# For the embedding layer. \"+ 1\" to account for the padding token.\n",
        "num_tokens = len(sentence_tokenizer.word_index) + 1\n",
        "embedding_dim = 128\n",
        "\n",
        "# For the output layer. The number of classes corresponds to the\n",
        "# number of possible tags.\n",
        "num_classes = len(tag_tokenizer.word_index) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8enX4iWaQ-mD"
      },
      "outputs": [],
      "source": [
        "# The set_seed call and kernel_initializer parameters are used here to\n",
        "# ensure you and I get the same results. To get random weight initializations,\n",
        "# remove them.\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "model = keras.Sequential()\n",
        "\n",
        "model.add(layers.Embedding(input_dim=num_tokens,\n",
        "                           output_dim=embedding_dim,\n",
        "                           input_length=MAX_LENGTH,\n",
        "                           mask_zero=True))\n",
        "\n",
        "model.add(layers.Bidirectional(layers.LSTM(128, return_sequences=True,\n",
        "                                           kernel_initializer=tf.keras.initializers.random_normal(seed=1))))\n",
        "\n",
        "model.add(layers.Dense(num_classes, activation='softmax',\n",
        "                       kernel_initializer=tf.keras.initializers.random_normal(seed=1)))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-RPX0V86btV"
      },
      "source": [
        "A few notes about the model summary:<br>\n",
        "\n",
        "The embedding layer **output** has three dimensions:\n",
        "- Batch size (it's showing as \"None\" because we didn't specify it upfront. We'll do it when we call *model.fit*).\n",
        "- Sequence length (the sequences are all the same length now after our padding step).\n",
        "- Embedding dimension.\n",
        "<br><br>\n",
        "\n",
        "The LSTM outputs a vector *twice* the size of what we specified because it's bidirectional. Recall from the slides that the outputs from the two LSTMs will be concatenated before going to the output layer.\n",
        "<br><br>\n",
        "\n",
        "The final layer's **output** also has three dimensions:\n",
        "- Batch size\n",
        "- Sequence length\n",
        "- Output dimension (the number of possible tags).\n",
        "\n",
        "The output will be a **sequence of probability distributions** for each input sequence. One probability distribution per tag.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2mAahG9U7lj"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0XDusUT-Uew"
      },
      "source": [
        "we'll use *early stopping* with some *patience* to halt training once validation loss stops improving.<br>\n",
        "\n",
        "The model will compare its output (sequences of softmax-generated probability distributions) against the one-hot encoded targets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0HuoNKrU8si"
      },
      "outputs": [],
      "source": [
        "es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "history = model.fit(x_train_padded, y_train_categoricals, epochs=20,\n",
        "                    batch_size=256, validation_data=(x_val_padded, y_val_categoricals),\n",
        "                    callbacks=[es_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSRTSU_PFF_5"
      },
      "source": [
        "Once our model is trained, we'll vectorize and pad the testing dataset. In the case of the labels, we'll also one-hot encode them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjBNMiGyaoRC"
      },
      "outputs": [],
      "source": [
        "# Preprocess the test data and test the model.\n",
        "x_test_seqs = sentence_tokenizer.texts_to_sequences(x_test)\n",
        "x_test_padded = keras.preprocessing.sequence.pad_sequences(x_test_seqs, padding='post', maxlen=MAX_LENGTH)\n",
        "\n",
        "y_test_seqs = tag_tokenizer.texts_to_sequences(y_test)\n",
        "y_test_padded = keras.preprocessing.sequence.pad_sequences(y_test_seqs, padding='post', maxlen=MAX_LENGTH)\n",
        "y_test_categoricals = keras.utils.to_categorical(y_test_padded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTJ3HvUxVGVK"
      },
      "outputs": [],
      "source": [
        "model.evaluate(x_test_padded, y_test_categoricals)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EnxAd_UbWpt"
      },
      "source": [
        "We can now use our model to tag sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpdBrMTKYC2o"
      },
      "outputs": [],
      "source": [
        "samples = [\n",
        "    \"Brown refused to testify.\",\n",
        "    \"Brown sofas are on sale.\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Pk-IzuDN7Fu"
      },
      "source": [
        "The function below takes a list of strings, tokenizes and pads them, then has the model tag them. Note that if a sentence is longer than MAX_LENGTH, it'll be truncated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJhZy6Qtbfgz"
      },
      "outputs": [],
      "source": [
        "def tag_sentences(sentences):\n",
        "  sentences_seqs = sentence_tokenizer.texts_to_sequences(sentences)\n",
        "  sentences_padded = keras.preprocessing.sequence.pad_sequences(sentences_seqs,\n",
        "                                                                maxlen=MAX_LENGTH,\n",
        "                                                                padding='post')\n",
        "\n",
        "  # The model returns a LIST of PROBABILITY DISTRIBUTIONS (due to the softmax)\n",
        "  # for EACH sentence. There is one probability distribution for each PoS tag.\n",
        "  tag_preds = model.predict(sentences_padded)\n",
        "\n",
        "  sentence_tags = []\n",
        "\n",
        "  # For EACH LIST of probability distributions...\n",
        "  for i, preds in enumerate(tag_preds):\n",
        "\n",
        "    # Extract the most probable tag from EACH probability distribution.\n",
        "    # Note how we're extracting tags for only the non-padding tokens.\n",
        "    tags_seq = [np.argmax(p) for p in preds[:len(sentences_seqs[i])]]\n",
        "\n",
        "    # Convert the sentence and tag sequences back to their token counterparts.\n",
        "    words = [sentence_tokenizer.index_word[w] for w in sentences_seqs[i]]\n",
        "    tags = [tag_tokenizer.index_word[t] for t in tags_seq]\n",
        "    sentence_tags.append(list(zip(words, tags)))\n",
        "\n",
        "  return sentence_tags\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epf-F8q79U8i"
      },
      "outputs": [],
      "source": [
        "tagged_sample_sentences = tag_sentences(samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1n8DlpC3fx78"
      },
      "outputs": [],
      "source": [
        "print(tagged_sample_sentences[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zu9RsRzPe1Ft"
      },
      "outputs": [],
      "source": [
        "print(tagged_sample_sentences[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gTzEc6Yd_Bt"
      },
      "source": [
        "So that's one way to build a PoS tagger. Industrial-strength taggers use a lot more data and these days, are powered by more sophisticated models which we'll learn about when we cover *transformers*."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "jY9ICurtUoDs"
      ],
      "name": "nlpdemystified-recurrent-neural-networks.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
